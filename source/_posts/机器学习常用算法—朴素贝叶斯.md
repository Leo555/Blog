---
title: 机器学习常用算法——朴素贝叶斯
date: 2017-01-09 21:27:24
categories: Machine Learning
tags:
- Python
- Algorithm
- Machine Learning
- scikit-learn
- GaussianNB
---
# 朴素贝叶斯

贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。

朴素贝叶斯分类是贝叶斯分类中最简单的一种。朴素贝叶斯的思想基础是这样的：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。通俗来说，就好比这么个道理，你在街上看到一个黑人，我问你你猜这哥们哪里来的，你十有八九猜非洲。为什么呢？因为黑人中非洲人的比率最高，当然人家也可能是美洲人或亚洲人，但在没有其它可用信息下，我们会选择条件概率最大的类别，这就是朴素贝叶斯的思想基础。


<!-- more -->

SVM 是一种分类方法。在这个算法中，我们将每个数据在 N 维空间中用点标出（N是你所有的特征总数），每个特征的值是一个坐标的值。
举个例子，如果我们只有身高和体重两个特征，我们会在二维空间中标出这两个变量，每个点有两个坐标（这些坐标叫做支持向量）。

现在，我们会找到将两组不同数据分开的一条直线。两个分组中距离最近的两个点到这条线的距离同时最优化。



## 原理



## SVM的优点

1. SVM学习问题可以表示为凸优化问题，因此可以利用已知的有效算法发现目标函数的全局最小值。而其他分类方法（如基于规则的分类器和人工神经网络）都采用一种基于贪心学习的策略来搜索假设空间，这种方法一般只能获得局部最优解。

2. 假设现在你是一个农场主，圈养了一批羊群，但为预防狼群袭击羊群，你需要搭建一个篱笆来把羊群围起来。但是篱笆应该建在哪里呢？你很可能需要依据牛群和狼群的位置建立一个“分类器”，比较下图这几种不同的分类器，我们可以看到SVM完成了一个很完美的解决方案。


<img src="http://img.blog.csdn.net/20131121105410546">
这个例子从侧面简单说明了SVM 使用非线性分类器的优势，而逻辑模式以及决策树模式都是使用了直线方法。


[代码地址](https://github.com/Leo555/scikit-learn_demo/tree/master/02LogisticRegression)

# 参考文献
[支持向量机通俗导论（理解SVM的三层境界）](http://blog.csdn.net/macyang/article/details/38782399/)
[支持向量机（SVM）算法](http://www.cnblogs.com/end/p/3848740.html)