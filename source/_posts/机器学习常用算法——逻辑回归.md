---
title: 机器学习常用算法——逻辑回归
date: 2017-01-06 21:27:24
categories: Algorithm
tags:
- Python
- Algorithm
- Machine Learning
- scikit-learn
- LogisticRegression
---
<img src="/assets/img/逻辑回归.jpg" alt="线性回归">
[比利时的人口增长数量图]

# 逻辑回归

首先，逻辑回归是一个分类算法而不是一个回归算法，该算法可根据已知的一系列因变量估计离散数值（比方说二进制数值 0 或 1 ，是或否，真或假），它通过将数据拟合进一个 **逻辑函数** 来预估一个事件出现的概率。因为它预估的是概率，所以它的输出值大小在 0 和 1 之间（正如所预计的一样）。
<!-- more -->
逻辑函数由于它的S形，有时也被称为sigmoid函数。

让我们再次通过一个简单的例子来理解这个算法。

假设你的朋友让你解开一个谜题。这只会有两个结果：你解开了或是你没有解开（离散值）。想象你要解答很多道题来找出你所擅长的主题。这个研究的结果就会像是这样：假设题目是一道十年级的三角函数题，你有 70% 的可能会解开这道题。然而，若题目是个五年级的历史题，你只有 30% 的可能性回答正确。这就是逻辑回归能提供给你的信息。

## 用途

逻辑回归主要用于分类，比如邮件分类，是否肿瘤、癌症诊断，用户性别判断，预测用户购买产品类别，判断评论是正面还是负面等。

逻辑回归的数学模型和求解都相对比较简洁，实现相对简单。通过对特征做离散化和其他映射，逻辑回归也可以处理非线性问题，是一个非常强大的分类器。因此在实际应用中，当我们能够拿到许多低层次的特征时，可以考虑使用逻辑回归来解决我们的问题。

## 加载数据(Data Loading)

我们假设输入是一个特征矩阵或者 csv 文件，我们使用 NumPy 来载入 csv 文件。
以下是从 UCI 机器学习数据仓库中下载的数据。

## 数据归一化(Data Normalization)

数据归一化是指把数字变成 （0,1）之间的小数。

大多数机器学习算法中的梯度方法对于数据的缩放和尺度都是很敏感的，在开始跑算法之前，我们应该进行归一化或者标准化的过程，这使得特征数据缩放到 0-1 范围中。scikit-learn 提供了归一化的方法：

## 特征选择(Feature Selection)

在解决一个实际问题的过程中，选择合适的特征或者构建特征的能力特别重要。这成为特征选择或者特征工程。
特征选择时一个很需要创造力的过程，更多的依赖于直觉和专业知识，并且有很多现成的算法来进行特征的选择。
下面的树算法(Tree algorithms)计算特征的信息量：

## 选择算法

大多数问题都可以归结为二元分类问题。这个算法的优点是可以给出数据所在类别的概率。



# 参考文献
[Logistic Regression 模型简介](http://tech.meituan.com/intro_to_logistic_regression.html)